{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "#Params\n",
    "datasetPath = \"../datasets/\"\n",
    "dataPath = \"./data/\"\n",
    "resultPath = \"./results/\"\n",
    "#filenames = [\"BerkeleyC\",\"BerkeleyJ\",\"Dune\",\"HIPAcc\",\"HSMGP\",\"JavaGC\"]\n",
    "filenames = [\"Apache\"]\n",
    "perf=\"perf\"\n",
    "\n",
    "\n",
    "#Params for sensistivity\n",
    "NBINS = 40 # Number of vertical bins for threshold\n",
    "NSUBS = 10 # Number of training sets to average on\n",
    "srm = 1 # Minimum sampling size\n",
    "srM=[]\n",
    "srs = []\n",
    "for k,filename in enumerate(filenames):\n",
    "    srM.append(int(subprocess.check_output(\"echo $(wc -l < \"+datasetPath+filename+\".csv)\", shell=True))) # Maximum sampling size\n",
    "    srs.append(srM[k]//100) # Sampling step between two iterations\n",
    "    \n",
    "#Params for Decision Tree\n",
    "#Default params\n",
    "classParamsDefault = {\n",
    "    \"criterion\":\"gini\",\n",
    "    \"splitter\":\"best\",\n",
    "    \"max_features\":None,\n",
    "    \"max_depth\":None,\n",
    "    \"min_samples_split\":2,\n",
    "    \"min_samples_leaf\":1,\n",
    "    \"min_weight_fraction_leaf\":0.,\n",
    "    \"max_leaf_nodes\":None,\n",
    "    \"class_weight\":None,\n",
    "    \"random_state\":None,\n",
    "    \"min_impurity_split\":1e-7,\n",
    "    \"presort\":False\n",
    "}\n",
    "regParamsDefault = {\n",
    "    \"criterion\":\"mse\",\n",
    "    \"splitter\":\"best\",\n",
    "    \"max_depth\":None,\n",
    "    \"min_samples_split\":2,\n",
    "    \"min_samples_leaf\":1,\n",
    "    \"min_weight_fraction_leaf\":0.,\n",
    "    \"max_features\":None,\n",
    "    \"random_state\":None,\n",
    "    \"max_leaf_nodes\":None,\n",
    "    \"min_impurity_split\":1e-7,\n",
    "    \"presort\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def newVersionFilename(path, filename):\n",
    "    # Get all the files in the {path} directory starting with {filename}\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f)) and f.startswith(filename+\"-\")]\n",
    "    files.sort(reverse=True)\n",
    "    # If no file yet\n",
    "    if len(files)==0:\n",
    "        return path+filename+\"-\"+str(1).zfill(4)\n",
    "    # Split the last one\n",
    "    splitted = files[0].split(\"-\")\n",
    "    # Get the last version\n",
    "    num = int(splitted[len(splitted)-1].split(\".\")[0])\n",
    "    # Return the full name with new version\n",
    "    return path+filename+\"-\"+str(num+1).zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sensitivityClassification(datasetPath, dataPath, filename, perf, NBINS, NSUBS, srm, srM, srs, params, thresholds=False):\n",
    "\n",
    "    d = pd.read_csv(datasetPath+filename+\".csv\") # Open dataset\n",
    "    d = d.sort_values(by=perf) # Sort it by perf to get threshold values\n",
    "    thresholds = [d[perf].iloc[i * d.shape[0]//NBINS] for i in range(1, NBINS)]\n",
    "\n",
    "    res = {\"sr\":[],\"t\":[],\"TN\":[],\"TP\":[],\"FN\":[],\"FP\":[]}\n",
    "\n",
    "    #Put limit so that the size of training set is not too small and not too big\n",
    "    for sr in range(srm+1,int(0.9*srM),srs):\n",
    "        for t in thresholds:\n",
    "            print(\"Computing for sr=%d and t=%.3f...\" % (sr, t))\n",
    "            #Set label to 1 if perf above the threshold\n",
    "            d[\"label\"] = 0\n",
    "            d.loc[d[perf] > t, \"label\"] = 1\n",
    "\n",
    "            TN = TP = FN = FP = 0 # Counters for classification results\n",
    "\n",
    "            clean = d.drop([\"perf\"],axis=1,errors=\"ignore\")\n",
    "            \n",
    "            #Prepare NSUBS stratified training sets and test sets\n",
    "            shuffle_split = StratifiedShuffleSplit(train_size=sr, n_splits=NSUBS)\n",
    "            \n",
    "            #Prepare the decision tree\n",
    "            c = tree.DecisionTreeClassifier(**params)\n",
    "\n",
    "            try:\n",
    "\n",
    "                for train_index, test_index in shuffle_split.split(clean,clean.label):\n",
    "                    \n",
    "                    #training the tree\n",
    "                    c.fit(clean.drop([\"label\"],axis=1).iloc[train_index], clean.label.iloc[train_index])\n",
    "                    \n",
    "                    #extracting prediction on the test set\n",
    "                    pred = c.predict(clean.drop([\"label\"],axis=1).iloc[test_index])\n",
    "                    dfTest = pd.DataFrame()\n",
    "                    dfTest[\"label\"] = clean.label.iloc[test_index]\n",
    "                    dfTest[\"pred\"] = pred\n",
    "                    \n",
    "                    #calculatingtrue and false negatives and positives\n",
    "                    TN += dfTest[(dfTest.label == 0) & (dfTest.pred == 0)].shape[0]\n",
    "                    TP += dfTest[(dfTest.label == 1) & (dfTest.pred == 1)].shape[0]\n",
    "                    FN += dfTest[(dfTest.label == 1) & (dfTest.pred == 0)].shape[0]\n",
    "                    FP += dfTest[(dfTest.label == 0) & (dfTest.pred == 1)].shape[0]\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "                break\n",
    "\n",
    "            res[\"sr\"].append(sr)\n",
    "            res[\"t\"].append(t)\n",
    "            res[\"TN\"].append(TN/NSUBS)\n",
    "            res[\"TP\"].append(TP/NSUBS)\n",
    "            res[\"FN\"].append(FN/NSUBS)\n",
    "            res[\"FP\"].append(FP/NSUBS)\n",
    "    \n",
    "    #Getting a new filename\n",
    "    newFilename = newVersionFilename(dataPath,filename)\n",
    "    #Saving the data\n",
    "    pd.DataFrame(res).to_csv(newFilename+\".csv\", index=False)\n",
    "    \n",
    "    #Saving the used params\n",
    "    paramsUsed = dict(params)\n",
    "    paramsUsed['file']=filename\n",
    "    paramsUsed['results']=newFilename+\".csv\"\n",
    "    \n",
    "    dfParamsUsed = pd.DataFrame.from_dict([paramsUsed])\n",
    "    \n",
    "    # If params list does not exists, create it\n",
    "    if not os.path.exists(dataPath+\"results-list.csv\"):\n",
    "        dfParamsUsed.to_csv(dataPath+\"results-list.csv\", index=False)\n",
    "    # If the list already exists, add the params used\n",
    "    else:\n",
    "        paramList = pd.read_csv(dataPath+\"results-list.csv\")\n",
    "        frames = [paramList, dfParamsUsed]\n",
    "        paramList = pd.concat(frames)\n",
    "        pd.DataFrame(paramList).to_csv(dataPath+\"results-list.csv\", index=False)\n",
    "        \n",
    "def sensitivityRegression(datasetPath, dataPath, filename, perf, NBINS, NSUBS, srm, srM, srs, params, thresholds=False):\n",
    "\n",
    "    d = pd.read_csv(datasetPath+filename+\".csv\") # Open dataset\n",
    "    d = d.sort_values(by=perf) # Sort it by perf to get threshold values\n",
    "    thresholds = [d[perf].iloc[i * d.shape[0]//NBINS] for i in range(1, NBINS)]\n",
    "\n",
    "    res = {\"sr\":[],\"t\":[],\"TN\":[],\"TP\":[],\"FN\":[],\"FP\":[]}\n",
    "\n",
    "    #Put limit so that the size of training set is not too small and not too big\n",
    "    for sr in range(srm+1,int(0.9*srM),srs):\n",
    "        for t in thresholds:\n",
    "            print(\"Computing for sr=%d and t=%.3f...\" % (sr, t))\n",
    "            d[\"label\"] = 0\n",
    "            d.loc[d[perf] > t, \"label\"] = 1\n",
    "\n",
    "            TN = TP = FN = FP = 0 # Counters for classification results\n",
    "\n",
    "            clean = d.drop([\"perf\"],axis=1,errors=\"ignore\")\n",
    "\n",
    "            #Prepare NSUBS stratified training sets and test sets\n",
    "            shuffle_split = StratifiedShuffleSplit(train_size=sr, n_splits=NSUBS)\n",
    "            \n",
    "            #Prepare the decision tree\n",
    "            c = tree.DecisionTreeRegressor(**params)\n",
    "\n",
    "            try:\n",
    "\n",
    "                for train_index, test_index in shuffle_split.split(clean,clean.label):\n",
    "                    \n",
    "                    #training the tree\n",
    "                    c.fit(clean.drop([\"label\"],axis=1).iloc[train_index], clean.label.iloc[train_index])\n",
    "                    \n",
    "                    #extracting prediction on the test set\n",
    "                    pred = c.predict(clean.drop([\"label\"],axis=1).iloc[test_index])\n",
    "                    dfTest = pd.DataFrame()\n",
    "                    dfTest[\"label\"] = clean.label.iloc[test_index]\n",
    "                    dfTest[\"pred\"] = pred\n",
    "                    \n",
    "                    #calculatingtrue and false negatives and positives\n",
    "                    TN += dfTest[(dfTest.label == 0) & (dfTest.pred == 0)].shape[0]\n",
    "                    TP += dfTest[(dfTest.label == 1) & (dfTest.pred == 1)].shape[0]\n",
    "                    FN += dfTest[(dfTest.label == 1) & (dfTest.pred == 0)].shape[0]\n",
    "                    FP += dfTest[(dfTest.label == 0) & (dfTest.pred == 1)].shape[0]\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "                break\n",
    "\n",
    "            res[\"sr\"].append(sr)\n",
    "            res[\"t\"].append(t)\n",
    "            res[\"TN\"].append(TN/NSUBS)\n",
    "            res[\"TP\"].append(TP/NSUBS)\n",
    "            res[\"FN\"].append(FN/NSUBS)\n",
    "            res[\"FP\"].append(FP/NSUBS)\n",
    "    \n",
    "    \n",
    "    #Getting a new filename\n",
    "    newFilename = newVersionFilename(dataPath,filename)\n",
    "    #Saving the data\n",
    "    pd.DataFrame(res).to_csv(newFilename+\".csv\", index=False)\n",
    "    \n",
    "    #Saving the used params\n",
    "    paramsUsed = dict(params)\n",
    "    paramsUsed['file']=filename\n",
    "    paramsUsed['results']=newFilename+\".csv\"\n",
    "    \n",
    "    dfParamsUsed = pd.DataFrame.from_dict([paramsUsed])\n",
    "    \n",
    "    # If params list does not exists, create it\n",
    "    if not os.path.exists(dataPath+\"results-list.csv\"):\n",
    "        dfParamsUsed.to_csv(dataPath+\"results-list.csv\", index=False)\n",
    "    # If the list already exists, add the params used\n",
    "    else:\n",
    "        paramList = pd.read_csv(dataPath+\"results-list.csv\")\n",
    "        frames = [paramList, dfParamsUsed]\n",
    "        paramList = pd.concat(frames)\n",
    "        pd.DataFrame(paramList).to_csv(dataPath+\"results-list.csv\", index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone the default params\n",
    "params = dict(classParamsDefault)\n",
    "\n",
    "#set some params\n",
    "params['criterion']=\"entropy\"\n",
    "params['min_samples_leaf']=2\n",
    "\n",
    "#set mulitple params configurations\n",
    "for max_leaf_nodes in [20,30,40,50]:\n",
    "    params['max_leaf_nodes']=2\n",
    "    \n",
    "    for k,filename in enumerate(filenames):\n",
    "        sensitivityClassification(datasetPath = datasetPath, dataPath = dataPath, filename = filename, perf = perf,\n",
    "                    NBINS = NBINS, NSUBS = NSUBS, srm = srm, srM = srM[k], srs = srs[k], params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
