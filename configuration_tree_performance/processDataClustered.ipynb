{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "#Params\n",
    "datasetPath = \"../datasets/\"\n",
    "dataPath = \"./data_clustered/\"\n",
    "resultPath = \"./results/\"\n",
    "filenames = [\"Apache\"]\n",
    "perf=\"perf\"\n",
    "\n",
    "n_lines=[]\n",
    "for k,filename in enumerate(filenames):\n",
    "    n_lines.append(int(subprocess.check_output(\"echo $(wc -l < \"+datasetPath+filename+\".csv)\", shell=True)))\n",
    "\n",
    "#Params for sensitivity\n",
    "NBINS = 40 # Number of vertical bins for threshold\n",
    "NSUBS = 10 # Number of training sets to average on\n",
    "srm = 1 # Minimum sampling size\n",
    "srM=[]\n",
    "srs = []\n",
    "for k,filename in enumerate(filenames):\n",
    "    srM.append(int(subprocess.check_output(\"echo $(wc -l < \"+datasetPath+filename+\".csv)\", shell=True))) # Maximum sampling size\n",
    "    srs.append(srM[k]//100) # Sampling step between two iterations\n",
    "\n",
    "    \n",
    "oracle = [0.2,0.5,0.8]\n",
    "\n",
    "#Params for Decision Tree\n",
    "#Default params\n",
    "treeParamsDefault = {\n",
    "    \"criterion\":\"gini\",\n",
    "    \"splitter\":\"best\",\n",
    "    \"max_features\":None,\n",
    "    \"max_depth\":None,\n",
    "    \"min_samples_split\":2,\n",
    "    \"min_samples_leaf\":1,\n",
    "    \"min_weight_fraction_leaf\":0.,\n",
    "    \"max_leaf_nodes\":None,\n",
    "    \"class_weight\":None,\n",
    "    \"random_state\":None,\n",
    "    \"min_impurity_split\":1e-7,\n",
    "    \"presort\":False\n",
    "}\n",
    "treeParams = treeParamsDefault\n",
    "# Modify params there\n",
    "treeParams['criterion']=\"entropy\"\n",
    "treeParams['min_samples_leaf']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def newVersionFilename(path, filename):\n",
    "    # Get all the files in the {path} directory starting with {filename}\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f)) and f.startswith(filename)]\n",
    "    files.sort(reverse=True)\n",
    "    # If no file yet\n",
    "    if len(files)==0:\n",
    "        return path+filename+\"-\"+str(1).zfill(4)\n",
    "    # Split the last one\n",
    "    splitted = files[0].split(\"-\")\n",
    "    # Get the last version\n",
    "    num = int(splitted[len(splitted)-1].split(\".\")[0])\n",
    "    # Return the full name with new version\n",
    "    return path+filename+\"-\"+str(num+1).zfill(4)\n",
    "\n",
    "def sensitivity(datasetPath, dataPath, filename, perf, NBINS, NSUBS, srm, srM, srs, treeParams, oracle=False):\n",
    "    \n",
    "    # Create a list of params used for the function\n",
    "    varParams = locals()\n",
    "    del varParams['treeParams']\n",
    "    del varParams['oracle']\n",
    "    varParams.update(treeParams)\n",
    "    \n",
    "    #If data fodler does not exists\n",
    "    if not os.path.exists(dataPath):\n",
    "        try:\n",
    "            os.makedirs(dataPath)\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    \n",
    "    perf_x264 = ['Watt', 'Energy', 'SSIM', 'PSNR', 'Speed', 'Size', 'Time']\n",
    "    perf_sac = ['compile-exit', 'compile-real', 'compile-user', 'compile-ioin', 'compile-ioout',\n",
    "                'compile-maxmem', 'compile-cpu', 'compile-size', 'run-exit',\n",
    "                'run-real', 'run-user', 'run-maxmem', 'run-cpu']\n",
    "\n",
    "    d = pd.read_csv(datasetPath+filename+\".csv\") # Open dataset\n",
    "    d = d.sort_values(by=perf) # Sort it by perf to get threshold values\n",
    "    if not oracle:\n",
    "        thresholds = [d[perf].iloc[i * d.shape[0]//NBINS] for i in range(1, NBINS)]\n",
    "    else:\n",
    "        for i in oracle:\n",
    "            if not os.path.exists(dataPath+str(i)):\n",
    "                try:\n",
    "                    os.makedirs(dataPath+str(i))\n",
    "                except OSError as exc: # Guard against race condition\n",
    "                    if exc.errno != errno.EEXIST:\n",
    "                        raise\n",
    "        thresholds = [d[perf].iloc[int(i * d.shape[0])] for i in oracle]\n",
    "    \n",
    "    res = {\"sr\":[],\"t\":[],\"TN\":[],\"TP\":[],\"FN\":[],\"FP\":[]}\n",
    "    \n",
    "    # Computation\n",
    "    for k,t in enumerate(thresholds):\n",
    "        if oracle:\n",
    "            res = {\"sr\":[],\"t\":[],\"TN\":[],\"TP\":[],\"FN\":[],\"FP\":[]}\n",
    "        for sr in range(srm,srM,srs):\n",
    "                print(\"Computing for sr=%d and t=%.3f...\" % (sr, t))\n",
    "                d[\"label\"] = 0\n",
    "                d.loc[d[perf] > t, \"label\"] = 1 # Label with the (current) oracle\n",
    "                clean = d.drop(perf_sac+perf_x264+[\"perf\"],axis=1,errors=\"ignore\")\n",
    "                subs = [clean.sample(sr) for i in range(NSUBS)] # Subsample trainsets\n",
    "                TN = TP = FN = FP = 0 # Counters for classification results\n",
    "                d[\"pred\"] = 0\n",
    "                for s in subs: # We cumulate results for each experiment and average later\n",
    "                    # MACHINE LEARNING PART\n",
    "                    # Settings are chosen to be the closest to J48 algorithm\n",
    "                    ##c = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf=2)\n",
    "                    c = tree.DecisionTreeClassifier(**treeParams)\n",
    "                    c.fit(s.drop([\"label\"],axis=1), s.label)\n",
    "                    # END OF LEARNING\n",
    "                    d[\"pred\"] = c.predict(clean.drop([\"label\"], axis=1)) # Get model's prediction\n",
    "                    TN += d[(d.label == 0) & (d.pred == 0)].shape[0]\n",
    "                    TP += d[(d.label == 1) & (d.pred == 1)].shape[0]\n",
    "                    FN += d[(d.label == 1) & (d.pred == 0)].shape[0]\n",
    "                    FP += d[(d.label == 0) & (d.pred == 1)].shape[0]\n",
    "                del d[\"pred\"] # Reset\n",
    "                # Push the results\n",
    "                res[\"sr\"].append(sr)\n",
    "                res[\"t\"].append(t)\n",
    "                res[\"TN\"].append(TN/NSUBS)\n",
    "                res[\"TP\"].append(TP/NSUBS)\n",
    "                res[\"FN\"].append(FN/NSUBS)\n",
    "                res[\"FP\"].append(FP/NSUBS)\n",
    "        # Save the result as csv\n",
    "        if oracle:\n",
    "            newFilename = newVersionFilename(dataPath+\"/\"+str(oracle[k])+\"/\",filename)+\".csv\"\n",
    "            pd.DataFrame(res).to_csv(newFilename, index=False)\n",
    "            # Save the params used\n",
    "            paramsUsed = varParams\n",
    "            paramsUsed['oracle']=oracle[k]\n",
    "            paramsUsed['file']=filename\n",
    "            paramsUsed['results']=newFilename\n",
    "            \n",
    "            dfParamsUsed = pd.DataFrame.from_dict([paramsUsed])\n",
    "            \n",
    "            # If params list does not exists, create it\n",
    "            if not os.path.exists(dataPath+\"/\"+str(oracle[k])+\"-results-list.csv\"):\n",
    "                dfParamsUsed.to_csv(dataPath+\"/\"+str(oracle[k])+\"-results-list.csv\", index=False)\n",
    "            # If the list already exists, add the params used\n",
    "            else:\n",
    "                paramList = pd.read_csv(dataPath+\"/\"+str(oracle[k])+\"-results-list.csv\")\n",
    "                frames = [paramList, dfParamsUsed]\n",
    "                paramList = pd.concat(frames)\n",
    "                pd.DataFrame(paramList).to_csv(dataPath+\"/\"+str(oracle[k])+\"-results-list.csv\", index=False)\n",
    "       \n",
    "    if not oracle:\n",
    "        newFilename = newVersionFilename(dataPath+\"/\",filename)+\".csv\"\n",
    "        pd.DataFrame(res).to_csv(newFilename, index=False)\n",
    "        # Save the params used\n",
    "        paramsUsed = varParams\n",
    "        paramsUsed['file']=filename\n",
    "        paramsUsed['results']=newFilename\n",
    "\n",
    "        dfParamsUsed = pd.DataFrame.from_dict([paramsUsed])\n",
    "\n",
    "        # If params list does not exists, create it\n",
    "        if not os.path.exists(dataPath+\"/results-list.csv\"):\n",
    "            dfParamsUsed.to_csv(dataPath+\"/results-list.csv\", index=False)\n",
    "        # If the list already exists, add the params used\n",
    "        else:\n",
    "            paramList = pd.read_csv(dataPath+\"/results-list.csv\")\n",
    "            frames = [paramList, dfParamsUsed]\n",
    "            paramList = pd.concat(frames)\n",
    "            pd.DataFrame(paramList).to_csv(dataPath+\"/results-list.csv\", index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of running sensitivity on many configurations\n",
    "treeParamsArray = []\n",
    "\n",
    "for max_features in [0.8,0.9]:\n",
    "    for min_samples_leaf in [1,2]:\n",
    "        treeParams = dict(treeParamsDefault)\n",
    "        treeParams['criterion']=\"entropy\"\n",
    "        treeParams['min_samples_leaf']=2\n",
    "\n",
    "        treeParams['max_features']=max_features\n",
    "        treeParams['min_samples_leaf']=min_samples_leaf\n",
    "\n",
    "\n",
    "        treeParamsArray.append(treeParams)\n",
    "            \n",
    "for treeParams in treeParamsArray:\n",
    "    for k,filename in enumerate(filenames):\n",
    "        #Machine learning part, using data from {filename} file in {datasetPath} folder and writing results in {dataPath} folder\n",
    "        sensitivity(datasetPath = datasetPath, dataPath = dataPath, filename = filename, perf = perf, NBINS = NBINS,\n",
    "                    NSUBS = NSUBS, srm = srm, srM = srM[k], srs = srs[k], treeParams=treeParams, oracle=oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
